import numpy as np
def f_prime(theta, w, x):
    # TODO rewrite in efficient numpy way
    running_sum = 0
    for i in range(x.shape[0]):
        running_sum += w[i] * (theta - x[i])
    return 2 * running_sum

def gradient_descent_quadratic(x, w, theta0, lr, num_steps):
    theta = theta0
    for _ in range(num_steps):
        theta = theta - lr * f_prime(theta, w, x)
    return theta

def get_expected(x, w):
    numerator = 0
    for i in range(x.shape[0]):
        numerator += w[i] * x[i]
    denominator = 0
    for i in range(x.shape[0]):
        denominator += w[i]
    
    return numerator / denominator

# AI NOTICE:
# the code below was generated by GPT-5 mini for testing my functions
# except for the get_expected print statement
# Example 1: weighted
x = np.array([0.5, 2.7, 3.4])
w = np.array([0.2, 0.5, 0.3])
theta0 = 0.0
lr = 0.1
num_steps = 50
theta = gradient_descent_quadratic(x, w, theta0, lr, num_steps)
print("theta (weighted):", theta)
print("expected: ", get_expected(x, w))

# Example 2: equal weights (should converge to mean of x)
x2 = np.array([0.1, 75.0, 10.4])
w2 = np.ones_like(x2)
theta2 = gradient_descent_quadratic(x2, w2, theta0, lr, num_steps)
print("theta (equal weights):", theta2)
print("expected: ", get_expected(x2, w2))