import matplotlib
matplotlib.use('TkAgg')
import numpy as np
import matplotlib.pyplot as plt

# AI NOTICE:
# data generated by GPT-5.1-Codex-Mini
# content of graph_data function generated by Google Gemini
# rest of code was handwritten by me

NOISY_LINEAR_DATA = np.array(
    [
        [0.0, 0.8986652764265913],
        [0.35, 2.533106490683809],
        [0.9, 2.9449908076608082],
        [1.8, 5.446423553816781],
        [2.3, 7.4843284808477035],
        [3.25, 10.320610881716519],
        [3.9, 12.490082031419233],
        [4.5, 13.374565157953986],
        [5.7, 18.261000972156694],
        [6.1, 17.85715946596521],
        [7.4, 22.41797759435344],
        [9.6, 28.139172873848224],
    ],
    dtype=float,
)

STEP_SIZE = 0.01
N = NOISY_LINEAR_DATA.shape[0]
EPOCHS = 10

def graph_data(slope, intercept):
    w = slope
    b = intercept

    x_data = NOISY_LINEAR_DATA[:, 0]
    y_data = NOISY_LINEAR_DATA[:, 1]

    x_line = np.array([min(x_data), max(x_data)]) 
    y_line = w * x_line + b

    plt.scatter(x_data, y_data, color='blue', label='Data')
    plt.plot(x_line, y_line, color='green', label=f'Manual Line: y = {w}x + {b}')

    plt.legend()
    plt.show()
    
def feature_extractor(x):
    return np.array([1, x])

def hypothesis(x, weights):
    return np.dot(weights, feature_extractor(x))

# def loss(x, y, weights):
#     return (hypothesis(x, weights) - y) ** 2

def gradient(weights):
    running_sum = 0
    for x, y in NOISY_LINEAR_DATA:
        running_sum += 2 * (hypothesis(x, weights) - y) * feature_extractor(x)
    return running_sum / N

def gradient_descent(weights):
    for _ in range(0, EPOCHS):
        step = STEP_SIZE * gradient(weights)
        weights -= step
    return weights

final_weights = gradient_descent(np.array([0.0, 0.0]))
graph_data(final_weights[1], final_weights[0])

# Observation: good model after very few epochs (as few as 6 with step size 0.01)
# Step size must be around 0.04 or smaller -- 0.05 doesn't fit good line even with 10000 epochs
    